{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43306f4d",
   "metadata": {},
   "source": [
    "# Домашнее задание: Генератор текста на базе Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de02c8",
   "metadata": {},
   "source": [
    "## 1. Архитектура модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541370b",
   "metadata": {},
   "source": [
    "### Создайте класс `GeneratorTransformer`, который авторегрессивно генерирует продолжение текста. Обучите его на книгах или каких-нибудь текстах, которые вы найдете в интернете"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c17222b",
   "metadata": {},
   "source": [
    "Создал в файле generator.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66d2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\DeppLearningUrfu\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer_path, max_length=128):\n",
    "        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        self.tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<pad>\"])\n",
    "        self.max_length = max_length\n",
    "\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "        raw_text = \"\\n\\n\".join(dataset[\"text\"])\n",
    "        tokens = self.tokenizer.encode(raw_text).ids\n",
    "\n",
    "        bos = self.tokenizer.token_to_id(\"<s>\")\n",
    "        eos = self.tokenizer.token_to_id(\"</s>\")\n",
    "\n",
    "        self.sequences = []\n",
    "        for i in range(0, len(tokens) - max_length, max_length):\n",
    "            chunk = tokens[i:i + max_length]\n",
    "            self.sequences.append([bos] + chunk + [eos])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.tensor(self.sequences[idx])\n",
    "        return {\n",
    "            \"input_ids\": seq[:-1],\n",
    "            \"target_ids\": seq[1:]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535d35cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 129]) torch.Size([1, 129])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = TextDataset(\"transformer_basics/mistral_tokenizer.json\", max_length=128)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch[\"input_ids\"].shape, batch[\"target_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbdaeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import GeneratorTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae229d",
   "metadata": {},
   "source": [
    "## 4. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83472784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, dataset, num_epochs=3, batch_size=8, lr=1e-4, save_path=\"generator.pt\"):\n",
    "    model = model.to(model.device)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        for batch in progress:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            target_ids = batch[\"target_ids\"].to(model.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                logits = model(input_ids)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress.set_postfix(loss=total_loss / (progress.n + 1))\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished. Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "        # тестовая генерация\n",
    "        model.eval()\n",
    "        print(\"Generated:\", model.generate(\"In the future\", max_out_tokens=50))\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{save_path}_epoch{epoch+1}.pt\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25639874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 687/687 [00:34<00:00, 19.63it/s, loss=6.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Loss: 6.8463\n",
      "Generated: In the future of symbolvert , in providedative wifeorous for the minor . Later and conditions wasv of theyley spread parking — tro announced . highlighted , within calledott , and yield are the international Vill to one named ofatch important becomes Bon writing history Som\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 687/687 [00:34<00:00, 19.86it/s, loss=6.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Loss: 5.9890\n",
      "Generated: In the future concept on 28484 Society , spiniviaedistics , an most Kil Ver , in 500reation to 1 , 1 , although in the originalfl , Dylan . \n",
      "\n",
      "\n",
      " The priorities = =\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 687/687 [00:34<00:00, 19.88it/s, loss=5.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Loss: 5.7130\n",
      "Generated: In the future most capture . When Beco is approachedmentanistent due to bemate in an honorberrygeon . Compet for amounts as a nightiba and fewnaments 's own Sweet of Nelson blest Museum as a species of the shle of them vocals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 687/687 [00:34<00:00, 19.99it/s, loss=5.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished. Loss: 5.5150\n",
      "Generated: In the future records into the left of the maximum to anboard and Fame . Reaga of the New Zealand such as the company of the Qufully , of this period ( tenons ) , once designed , Christathens SAels a troops a spiritualau Pan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 687/687 [00:34<00:00, 19.99it/s, loss=5.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished. Loss: 5.3634\n",
      "Generated: In the future of the valley . Its ends because siming had been made being think in the Paris and him to a opening . However , \" He translate on August , \" Ithe interests rayly lular 's evident worked of most involved the cap , but\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 687/687 [00:34<00:00, 19.98it/s, loss=5.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished. Loss: 5.2395\n",
      "Generated: In the future of the Detroit after by fish to its predecessial crAL , in an 159 . The following order reached the Spanish to the alminated Bay of the roadensive , the bottom . \n",
      "\n",
      "\n",
      " Beyon and following episode director\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 687/687 [00:34<00:00, 19.88it/s, loss=5.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished. Loss: 5.1347\n",
      "Generated: In the future of its words , a heart decided in 2ndspian Tower the subsequentong , the 3 million years of 1971 Medal in the Oriior School . After a result round that season to Twain later became a General arg\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer.from_file(\"transformer_basics/mistral_tokenizer.json\")\n",
    "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<pad>\"])\n",
    "\n",
    "dataset = TextDataset(\"transformer_basics/mistral_tokenizer.json\", max_length=128)\n",
    "\n",
    "model = GeneratorTransformer(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    d_ff=512,\n",
    "    num_layers=4,\n",
    "    pad_token_id=tokenizer.token_to_id(\"<pad>\"),\n",
    "    bos_token_id=tokenizer.token_to_id(\"<s>\"),\n",
    "    eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",
    "    max_len=128,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "train_model(model, dataset, num_epochs=7, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
